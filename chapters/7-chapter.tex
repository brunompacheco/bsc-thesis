% ----------------------------------------------------------
\chapter{Conclusion}\label{ch:conclusion}
% ----------------------------------------------------------

An in-depth study was carried out with two novel approaches in the big area of deep learning: \gls{PINN} and {DEQ}.
Both provide a deeper connection between deep learning and other areas of knowledge, such as differential equations, optimization, and numerical analysis.
\gls{PINN}s provide an efficient approach to train deep learning models on problems involving physical phenomena.
\gls{DEQ}s are a promising new architecture that can provide a larger representational power with a small number of parameters.
This work proposed an application that combines both: using \gls{PIDEQ}s to solve \gls{IVP}s of \gls{ODE}s.
For this, we successfully studied, implemented and tested several \gls{PIDEQ} models.

To the best of our knowledge, this is the first study on physics regularization (or any gradient regularization) for \gls{DEQ}s.
In fact, we have not found any published result reporting higher-order derivatives of these models, as \textcite{Bai2019} only proposed an analytical solution for the first derivative.
The requirement for higher-order derivatives imposes limitations to the model, namely the use of a differentiable solver to compute the first derivative.

To validate the implementation of the model, we trained \gls{PIDEQ}s to solve \gls{IVP}s of the Van der Pol oscillator, a well-known, \gls{ODE}-governed system.
The experimental results showed that differentiating through the solver used for computing the first derivative did not have a big impact in the speed of training.
Computing the equilibrium (forward pass) is still the most costly operation and the biggest difference in comparison to \gls{PINN}s.
We hypothesize that larger models and more complex problems may result in harder-to-compute derivatives, as it may be harder to find equilibria efficiently.
This would change not only the forward pass but also the Jacobian used by the backward pass, therefore, requiring more iterations of the differentiable solver (simple iteration method) or even a more robust solver.

Comparing \gls{PIDEQ} results with \gls{PINN} models showed that the former has a larger approximation error and slower training.
Still, both presented very small errors in the proposed problems, to the point of being almost undistinguishable visually.
These results indicate that the inner structure of \gls{DEQ} models is not very useful for learning to approximate the target solution.

% the task is about extracting complex functions out of low-dimensional data, not usual, given the current trends of extracting simpler functions out of high-dimensional data

\section{Outlook}

Given that \gls{DEQ}s approximate infinite-depth models, it is reasonable to imagine that they are more effective in problems that benefit from deeper models, whereas in our problem, even a shallow deep feedforward network was able to properly approximate the target function.
Therefore, it is natural that a next step is to apply the proposed model to more complex problems.
The four-tanks system [REFTO Johansson, 2000; Gatzke, 2000], which was better solved with deeper networks in the approach of textcite[REFTO Antonello, 2021], could benefit from \glspl{PIDEQ}.
Also, the proposed model could easily be generalized to partial differential equations, and then compared to the original \gls{PINN} of textcite[REFTO Raissi, 2019].

A variation of the \gls{IVP} as discussed here is to train a model that can provide approximate solutions for a set of initial conditions, as discussed in textcite[REFTO Antonelo, 2021; Arnold, 2021].
This enables the efficient use of \glspl{PINN} for control problems.
Being able to train models for this problem with less parameters can be very useful in the sense that it increases explainability and, thus, robustness of the controller.

Lastly, another way to make \glspl{PIDEQ} models more competitive is to improve their efficiency.
In theory, the second (and any higher-order) derivative of \glspl{DEQ} can be implicitly computed, without the necessity of differentiating the solver.
This has the potential to significantly save training time, mainly for larger \glspl{DEQ}, by allowing the use of efficient solvers for computing the first derivative of the output.

