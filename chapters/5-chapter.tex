% ----------------------------------------------------------
\chapter{Deep Equilibrium Models}\label{ch:deq}
% ----------------------------------------------------------

This chapter is dedicated to lay out the foundations of the model architecture that is the central investigation point of this work.
\gls{DEQ}s have been proposed by \textcite{Bai2019} and \textcite{Ghaoui2019}, the latter naming them \emph{implicit models}.
In this chapter, we follow the notation of the former.

Furthermore, one of the greatest challenges in working with \gls{DEQ}s is that, by their implicit nature, they do not fit perfectly well with current deep learning tools.
Therefore, to better understand the nuances and challenges that this family of models present during the experiments, a good share of attention is dedicated to the specificities of performing back propagation with \gls{DEQ}s.

\section{Introduction and Definition}

In Chapter \ref{ch:deep-learning}, the intuition behind a deep learning model was introduced, that is, to model complex features through the composition of simple-yet-non-linear parametrized functions.
Besides the network defined in \ref{sec:neural-nets} (which is the base for \gls{PINN}s, as shown in chapter \ref{ch:pinn}), many other deep learning model architectures have been proposed over the years.
    Some of the architectures with the most surprising results involve composing the models with the same function applied multiple times, i.e., following the notation of chapter \ref{ch:deep-learning}, instead of defining the model as $f_{\gls{param}}=f_{\gls{param}}^{[L]}\circ \cdots \circ f_{\gls{param}}^{[1]}$, these architectures suggest a model similar to $f_{\gls{param}}=f_{\gls{param}}^{[1]}\circ \cdots \circ f_{\gls{param}}^{[1]}$.
Inspired by this, \textcite{Bai2019} takes a step further, defining the model with a (possibly) infinite stack of the same function, which was named \gls{DEQ}.

Let us recall the definition of a deep learning model as proposed in equation \eqref{eq:dl-model}, but imagine it has an infinite number of layers (infinite depth).
Of course, if each $f^{[i]}$ is a different function (with different parameters), then this would be impossible to fit in memory.
Therefore, let us assume that $f_{\gls{param}}^{[i]}=f_{\gls{param}}^{[EQ]},\forall i$, i.e.,
\begin{equation*}
\begin{split}
    z^{[0]} &= x \\
    z^{[i]} &= f_{\gls{param}}^{[EQ]}(z^{[i-1]}), \forall i\ge 1
,\end{split}
\end{equation*}
in which the output would be $z^{\star} = z^{[\infty]}$.
If this iterative process converges, that is, if there is a number $N$ such that $\forall i\ge N,\,z^{[i]}\approx z^{[i+1]}$, then the output $z^{\star}\approx z^{[N]}$ is well-defined, and it is true that  \[
    z^{\star} = f_{\gls{param}}\left( z^{\star} \right) 
.\] 
One can say that $z^{\star}$ is an \emph{equilibrium point} of $f_{\gls{param}}^{[EQ]}$.
Therefore, the output of a well-behaved (i.e., one that respects the restrictions above presented) infinite-depth deep learning model can be computed by finding its equilibrium point.

The model proposed by \textcite{Bai2019} has a slight change in how it handles the input, feeding the input vector at each layer of the model. More precisely, we can say that a \gls{DEQ} of the form
\begin{align*}
    \bm{f}_{\gls{param}}: \R^{n} &\longrightarrow \R^{m} \\
    \bm{x} &\longmapsto \bm{f}_{\gls{param}}(\bm{x}) = \bm{z}^{\star}
\end{align*}
defines its output as the equilibrium point of a function $\bm{f}_{\gls{param}}^{[EQ]}:\R^{n+m}\to \R^{m}$
\begin{equation}\label{eq:z-star}
    \bm{z}^{\star} = \bm{f}_{\gls{param}}^{[EQ]}\left( \bm{x},\bm{z}^{\star} \right) 
.\end{equation}

\section{Forward}

The most simple way to perform the forward pass of a \gls{DEQ}, i.e., to compute the output of the model given an input, is to iterate the application of the equilibrium function $\bm{f}_{\gls{param}}^{[EQ]}$ until the current value is close enough to the previous.
More specifically, given an input $\bm{x}$ and an initial guess $\bm{z}^{[0]}$, the procedure is to update the equilibrium guess $\bm{z}^{[i]}$ by \[
    \bm{z}^{[i]} = \bm{f}_{\gls{param}}^{[EQ]}(\bm{x}, \bm{z}^{[i-1]})
\] until $\|\bm{z}^{[i]}-\bm{z}^{[i+1]}\|$ is small enough.
This approach is the \emph{simple iteration} method\cite{suli_introduction_2003}.
Even though this approach is very intuitive given our derivation of a \gls{DEQ} from an infinite-depth model, it is quite limited.
First because it can be quite slow, i.e., it can take many iterations until convergence is achieved, being very sensitive to the starting point.
But mostly because this approach only finds equilibrium points if the function of interest is a contraction between the starting point and the equilibrium point\cite{suli_introduction_2003}.

This limitation can be easily visualized by trying to use the simple iteration method to find the equilibrium of $f(z) = 2z-1$.
The function clearly has an equilibrium at $z=1$.
Yet, at any starting point \emph{except} the equilibrium, the simple iteration method will diverge.

Luckily, we know from equation \eqref{eq:z-star} that the equilibrium point, for a given input, is also the root of a function $\bm{g}_{\bm{x}}(\bm{z}) = \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}) - \bm{z}$.
This means that using any root-finding algorithm on $\bm{g}_{\bm{x}}$ yields $\bm{z}^{\star}$, the desired output.
Perhaps the most classical root-finding algorithm is \emph{Newton's method}, which proposes to iterate over the solution space given \[
    \bm{z}^{[i+1]} = \bm{z}^{[i]} - \left( \frac{d \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}^{[i]})}{d\bm{z}} \right)^{-1} \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}^{[i]})
,\] 
in which $\frac{d \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}^{[i]})}{d\bm{z}}$ represents the Jacobian of  $\bm{f}_{\gls{param}}^{[EQ]}$ with respect to $\bm{z}$\footnotemark.
\footnotetext{To avoid the computational burden of inverting the Jacobian matrix, it is usual that the iteration focuses instead in solving $ \frac{d \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}^{[i]})}{d\bm{z}} \left(\bm{z}^{[i+1]} - \bm{z}^{[i]}\right) = -\bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}^{[i]})$ instead.}
Newton's method not only has guaranteed convergence for a broader class of functions in comparison to simple iteration, but also converges much faster\cite{suli_introduction_2003}.

\subsection{Practical Considerations}

Most modern algorithms that help us find the desired equilibrium point are either modifications of the simple iteration algorithm (e.g., Anderson Acceleration\cite{walker_anderson_2011}) or modifications of Newton's method (e.g., Broyden's method\cite{broyden_class_1965}).
Nevertheless, all these methods require an initial guess $\bm{z}^{[0]}$ and a tolerance $\epsilon>0$. 
The initial guess, or starting point, is clearly necessary, as it is natural for iterative procedures, and is usual to find it as $\bm{z}^{[0]}\gets \bm{0}$ by default.
The tolerance is necessary to define a stopping condition for the algorithm, when the approximation for the equilibrium point is "good enough", i.e., if $\|\bm{z}^{[i]}-\bm{z}^{[i+1]}\|<\epsilon$ then it is considered that the equilibrium has been reached.
Furthermore, it is also usual to define a limit for the number of iterations, avoiding that the algorithms run for an indefinite amount of time.

\subsection{Jacobian Regularization}

Two common empirical findings of \gls{DEQ} applications are that they are 1) unstable to architectural choices \cite{bai_stabilizing_2021} and 2) increasingly slower over training iterations \cite{Bai2019,winston_monotone_2020}.
This is a direct implication of the equilibrium-finding nature of the forward pass, which relies heavily on the behavior of $\bm{f}_{\gls{param}}^{[EQ]}$.
Intuitively, the complexity of $\bm{f}_{\gls{param}}^{[EQ]}$, which depends heavily on the architecture and is expected to increase during training, makes it harder for the root-finding algorithm to converge.

In a very recent work, \textcite{bai_stabilizing_2021} discussed how the Jacobian of $\bm{f}_{\gls{param}}^{[EQ]}$ with respect to $\bm{z}$ is related to both problems.
The authors propose, then, to penalize large values in this Jacobian during training and show how this increases robustness and convergence speed of \gls{DEQ}s, reducing training and inference times.
More specifically, they propose to compute the Frobenius norm\footnotemark of the Jacobian of $\bm{f}_{\gls{param}}^{[EQ]}$ and add it as a regularization term to the cost function (see sec. \ref{sec:regularization}).
\footnotetext{The Frobenius norm of a matrix $A$ can be written $\|A\|_F=\sqrt{\sum_{i,j=1}^{n} |a_{i,j}|^2} $.}

\section{Backward}

It was shown in sec. \ref{sec:backprop} that, in order to use a gradient descent algorithm to train a deep learning model, one must compute the derivatives of the cost function with respect to the model's parameters.
In the case of \gls{DEQ}s, this computation is not straight-forward.
Given $\bm{f}_{\gls{param}}:\R^{n}\to \R^{m}$ a \gls{DEQ} as defined above, computing the derivative of a cost function $J:\Omega\to \R$ with respect to the parameters can be seen as \[
    \nabla_{\gls{param}} J\left( \gls{param} \right) = \nabla_{\bm{f}_{\gls{param}}} J \frac{d \bm{f}_{\gls{param}}(\bm{x})}{d\gls{param}}
.\] 
At the same time, we know that the output of a model is given by an equilibrium point of $\bm{f}_{\gls{param}}^{[EQ]}$, which is computed using a root finding method, i.e., \[
    \bm{f}_{\gls{param}}(\bm{x}) = RootFind(\bm{g}_{\bm{x}}, \bm{z}^{[0]})
,\] where $\bm{g}_{\bm{x}}$ is a function so that $\bm{g}_{\bm{x}}(\bm{z}) = \bm{f}_{\gls{param}}^{[EQ]}(\bm{x},\bm{z}) - \bm{z}$.
Therefore, computing the derivatives of $\bm{f}_{\gls{param}}$ directly, requires the computation of the derivatives of the root-finding algorithm.
Yet, not only root-finding algorithms need not be differentiable, but even those that are may impose an enormous computational burden to compute the actual derivatives.
To illustrate the point, if we restrain ourselves to using the simple iteration method as the root-finding algorithm, we can apply the chain rule and decompose the derivative in computing the derivative of $\bm{f}_{\gls{param}}^{[EQ]}$ for as many times as there were iterations until convergence, which can make practical applications impossible.

Luckily, we can exploit the fact that the output of the model ($\bm{f}_{\gls{param}}\left( \bm{x} \right) = \bm{z}^{\star}$) is an equilibrium point of $\bm{f}_{\gls{param}}^{[EQ]}$.
This implies that, in a neighborhood of the input vector $\bm{x}$, $\bm{f}_{\gls{param}}$ is a \emph{parametrization} of \bm{z} with respect to $\bm{x}$, i.e., \[
    \bm{f}_{\gls{param}}^{[EQ]}\left( \bm{x},\bm{f}_{\gls{param}}(\bm{x}) \right) -\bm{f}_{\gls{param}}\left( \bm{x} \right) = \bm{0}
\] is true.


- recall backprop
- show how it plays out for DEQs
- root-finding need not be differentiable
- model is a parametrization of fEQ
- implicit function theorem

\subsection{Implementation}

- in practice, computing the inverse is expensive
- luckily, we just need the vjp
- turns vjp can be found through root finding!

