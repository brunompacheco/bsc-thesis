% ----------------------------------------------------------
\chapter{Deep Equilibrium Models}\label{ch:deq}
% ----------------------------------------------------------

This chapter is dedicated to lay out the foundations of the model architecture that is the central investigation point of this work.
\gls{DEQ}s have been proposed by textcite[REFTO Bai, 2019] and textcite[REFTO Ghaoui, 2019], the latter naming them \emph{implicit models}.
In this chapter, we follow the notation of the former.

Furthermore, one of the greatest challenges in working with \gls{DEQ}s is that, by their implicit nature, they do not fit perfectly well with current deep learning tools.
Therefore, to better understand the nuances and challenges that this family of models present during the experiments, a good share of attention is dedicated to the specificities of performing back propagation with \gls{DEQ}s.

\section{Introduction and Definition}

In chapter \ref{ch:deep-learning}, the intuition behind a deep learning model was introduced, that is, to model complex features through the composition of simple-yet-non-linear parametrized functions.
Besides the network defined in \ref{sec:neural-nets} (which is the base for \gls{PINN}s, as shown in chapter \ref{ch:pinn}), many other deep learning model architectures have been proposed over the years.
Some of the architectures with the most surprising results involve composing the models with the same function applied multiple times, i.e., following the notation of chapter \ref{ch:deep-learning}, instead of defining the model as $\bm{f}_{\gls{param}}=\bm{f}_{\gls{param}}^{[L]}\circ \cdots \circ \bm{f}_{\gls{param}}^{[1]}$, these architectures suggest a model similar to $\bm{f}_{\gls{param}}=\bm{f}_{\gls{param}}^{[1]}\circ \cdots \circ \bm{f}_{\gls{param}}^{[1]}$.
Inspired by this, textcite[REFTO Bai, 2019] takes a step further, defining the model with a (possibly) infinite stack of the same function, which was named \gls{DEQ}.

- then, have proposed to take another step in this direction, defining the model with a (possibly) infinite stack of the same function, which was named \gls{DEQ} or \emph{implict model}
- one of the greatest advantages of this

\section{Forward}

\subsection{Jacobian Regularization}

\section{Backward}

\section{Implementation}

