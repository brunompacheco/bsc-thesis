% ----------------------------------------------------------
\chapter{Deep Equilibrium Models}\label{ch:deq}
% ----------------------------------------------------------

This chapter's focus is to define the deep learning model that is the central investigation point of this work.
\glspl{DEQ} have been proposed by \textcite{Bai2019} and \textcite{Ghaoui2019}, the latter naming them \emph{implicit models}.
In this chapter, we follow the notation of the former.

Furthermore, one of the greatest challenges in working with \gls{DEQ}s is that, by their implicit nature, they do not fit perfectly well with current deep learning tools.
Therefore, to better understand the nuances and challenges that this family of models present during the experiments, a good share of attention is dedicated to the specificities of performing back propagation with \gls{DEQ}s.

\section{Introduction and Definition}\label{sec:deq-definition}

In Chapter \ref{ch:deep-learning}, the intuition behind a deep learning model was introduced, that is, to model complex features through the composition of simple-yet-non-linear parametrized functions.
Besides the network defined in Section \ref{sec:neural-nets} (which is the base for \gls{PINN}s, as shown in Chapter \ref{ch:pinn}), many other deep learning model architectures have been proposed over the years.
    Some of the architectures with the most surprising results involve composing the models with the same function applied multiple times, i.e., following the notation of Chapter \ref{ch:deep-learning}, instead of defining the model as a composition similar to $D_{\gls{param}}=f_{\gls{param}}^{[L]}\circ \cdots \circ f_{\gls{param}}^{[1]}$, these architectures suggest a model similar to $D_{\gls{param}}=f_{\gls{param}}^{[1]}\circ \cdots \circ f_{\gls{param}}^{[1]}$.
Inspired by this, \textcite{Bai2019} take a step further, defining the model with a (possibly) infinite stack of the same function, which was named \gls{DEQ}.

For the definition of a \gls{DEQ} $D^{EQ}_{\gls{param}}$, let us start with the definition of a deep learning model as proposed in Equation \eqref{eq:dl-model}, but imagine it having an infinite number of layers (infinite depth).
Of course, if each $f^{[i]}_{\gls{param}}$ is a different function (with different parameters), then this would be impossible to fit in memory.
Therefore, let us assume that $f_{\gls{param}}^{[i]}=f_{\gls{param}},\forall i$, i.e.,
\begin{equation*}
\begin{split}
    z^{[0]} &= x \\
    z^{[i]} &= f_{\gls{param}}(z^{[i-1]}), \forall i\ge 1
,\end{split}
\end{equation*}
in which the output would be \[
D_{\gls{param}}^{EQ}(x) = z^{\star} = z^{[\infty]}
.\]
If this iterative process converges, that is, if there is a number $N$ such that $\forall i\ge N,\,z^{[i]}\approx z^{[i+1]}$, then the output $z^{\star}\approx z^{[N]}$ is well-defined, and it is also true that  \[
    z^{\star} = f_{\gls{param}}\left( z^{\star} \right) 
.\] 
One can say that $z^{\star}$ is an \emph{equilibrium point} of $f_{\gls{param}}$.
Therefore, the output of a well-behaved (i.e., one that respects the restrictions above presented) infinite-depth deep learning model can be computed by finding its equilibrium point.

The model proposed by \textcite{Bai2019} has a slight change in how it handles the input, feeding the input vector at each layer of the model.
This means that the equilibrium function $\bm{f}_{\gls{param}}$ has as arguments both an input vector $\bm{x}$ and a state vector $\bm{z}$.
More precisely, we can say that a \gls{DEQ} of the form
\begin{align*}
    D^{EQ}_{\gls{param}}: \R^{n} &\longrightarrow \R^{m} \\
    \bm{x} &\longmapsto D^{EQ}_{\gls{param}}(\bm{x}) = \bm{z}^{\star}
\end{align*}
defines its output as the equilibrium point of a function $\bm{f}_{\gls{param}}:\R^{n+m}\to \R^{m}$ for a given input, 
\begin{equation}\label{eq:z-star}
    \bm{z}^{\star} = \bm{f}_{\gls{param}}\left( \bm{x},\bm{z}^{\star} \right) 
.\end{equation}

\section{Forward}\label{sec:deq-forward}

The simplest way to perform the forward pass of a \gls{DEQ}, i.e., to compute the output of the model given an input, is to iterate the application of the equilibrium function $\bm{f}_{\gls{param}}$ until the current value is close enough to the previous.
More specifically, given an input $\bm{x}$ and an initial guess $\bm{z}^{[0]}$, the procedure is to update the equilibrium guess $\bm{z}^{[i]}$ by \[
    \bm{z}^{[i]} = \bm{f}_{\gls{param}}(\bm{x}, \bm{z}^{[i-1]})
\] until $\|\bm{z}^{[i]}-\bm{z}^{[i-1]}\|$ is small enough.
This approach is the \emph{simple iteration} method~\cite{suli_introduction_2003}.
Even though this approach is very intuitive given our derivation of a \gls{DEQ} from an infinite-depth model, it is quite limited.
First because it can be quite slow, i.e., it can take many iterations until convergence is achieved, being very sensitive to the starting point.
But mostly because this approach only finds equilibrium points if the function of interest is a contraction between the starting point and the equilibrium point~\cite{suli_introduction_2003}.

This limitation can be easily visualized by trying to use the simple iteration method to find the equilibrium of $f(z) = 2z-1$.
The function clearly has an equilibrium at $z=1$.
Yet, at any starting point \emph{except} the equilibrium, the simple iteration method will diverge.

Luckily, we know from Equation \eqref{eq:z-star} that the equilibrium point, for a given input, is also the root of a function $\bm{g}_{\bm{x}}(\bm{z}) = \bm{f}_{\gls{param}}(\bm{x},\bm{z}) - \bm{z}$.
This means that using any root-finding algorithm on $\bm{g}_{\bm{x}}$ yields $\bm{z}^{\star}$, the desired output.
Perhaps the most classical root-finding algorithm is \emph{Newton's method}, which proposes to iterate over the solution space given \[
    \bm{z}^{[i+1]} = \bm{z}^{[i]} - \left( \frac{d \bm{f}_{\gls{param}}(\bm{x},\bm{z}^{[i]})}{d\bm{z}}  \right)^{-1} \bm{f}_{\gls{param}}(\bm{x},\bm{z}^{[i]})
,\] 
in which $\frac{d} {d\bm{z}} \bm{f}_{\gls{param}}(\bm{x},\bm{z}^{[i]})$ represents the Jacobian of  $\bm{f}_{\gls{param}}$ with respect to $\bm{z}$\footnotemark.
\footnotetext{To avoid the computational burden of inverting the Jacobian matrix, it is usual that the iteration focuses on solving $ \frac{d \bm{f}_{\gls{param}}(\bm{x},\bm{z}^{[i]})}{d\bm{z}} \left(\bm{z}^{[i+1]} - \bm{z}^{[i]}\right) = -\bm{f}_{\gls{param}}(\bm{x},\bm{z}^{[i]})$ instead.}
Newton's method not only has guaranteed convergence for a broader class of functions in comparison to simple iteration, but also converges much faster~\cite{suli_introduction_2003}.

\subsection{Practical Considerations}

Most modern algorithms that help us find the desired equilibrium point are either modifications of the simple iteration algorithm (e.g., Anderson Acceleration~\cite{walker_anderson_2011}) or modifications of Newton's method (e.g., Broyden's method~\cite{broyden_class_1965}).
Nevertheless, all these methods require an initial guess $\bm{z}^{[0]}$ and a tolerance $\varepsilon>0$. 
The initial guess, or starting point, is clearly necessary, as it is natural for iterative procedures, and is usual to find it as $\bm{z}^{[0]}\gets \bm{0}$ by default.
The tolerance is necessary to define a stopping condition for the algorithm, when the approximation for the equilibrium point is ``good enough,'' i.e., if $\|\bm{z}^{[i]}-\bm{z}^{[i-1]}\|<\varepsilon$ then it is considered that the equilibrium has been reached.
Furthermore, it is also usual to define a limit for the number of iterations, avoiding that the algorithms run for an indefinite amount of time.

\subsection{Jacobian Regularization}\label{sec:deq-jac-reg}

Two common empirical findings of \gls{DEQ} applications are that they are 1) unstable to architectural choices \cite{bai_stabilizing_2021} and 2) increasingly slower over training iterations \cite{Bai2019,winston_monotone_2020}.
This is a direct implication of the equilibrium-finding nature of the forward pass, which relies heavily on the behavior of $\bm{f}_{\gls{param}}$.
Intuitively, the complexity of $\bm{f}_{\gls{param}}$, which depends heavily on the architecture and is expected to increase during training, makes it harder for the root-finding algorithm to converge.

In a very recent work, \textcite{bai_stabilizing_2021} discussed how the Jacobian of $\bm{f}_{\gls{param}}$ with respect to $\bm{z}$ is related to both problems.
The authors propose, then, to penalize large values in this Jacobian during training and show how this increases robustness and convergence speed of \gls{DEQ}s, reducing training and inference times.
More specifically, they propose to compute the Frobenius norm\footnotemark of the Jacobian of $\bm{f}_{\gls{param}}$ and add it as a regularization term to the cost function (see Sec. \ref{sec:regularization}).
\footnotetext{The Frobenius norm of a matrix $A$ can be written $\|A\|_F=\sqrt{\sum_{i,j=1}^{n} |a_{i,j}|^2} $.}

\section{Backward}\label{sec:deq-backward}

It was shown in Sec. \ref{sec:backprop} that, in order to use a gradient descent algorithm to train a deep learning model, one must compute the derivatives of the cost function with respect to the model's parameters.
In the case of \gls{DEQ}s, this computation is not straight-forward.
Given $D^{EQ}_{\gls{param}}:\R^{n}\to \R^{m}$ a \gls{DEQ} as defined above, computing the derivative of a cost function $J:\Omega\to \R$ with respect to the parameters can be seen as \[
    \nabla_{\gls{param}} J\left( \gls{param} \right) = \nabla_{\bm{z}^{\star}} J(D^{EQ}_{\gls{param}}(\bm{x})) \frac{d D^{EQ}_{\gls{param}}(\bm{x})}{d\gls{param}}
.\] 
At the same time, we know that the output of a model is given by an equilibrium point of $\bm{f}_{\gls{param}}$, which is computed using a root finding method as \[
    D^{EQ}_{\gls{param}}(\bm{x}) = RootFind(\bm{g}, \bm{z}^{[0]})
,\] where $\bm{g}$ is the function $\bm{g}(\bm{x},\bm{z}) = \bm{f}_{\gls{param}}(\bm{x},\bm{z}) - \bm{z}$.
Therefore, directly computing the derivatives of $D^{EQ}_{\gls{param}}$ requires the computation of the derivatives of the root-finding algorithm.
Yet, not only root-finding algorithms are not usually differentiable, but even those that are differentiable may have an enormous computational cost to compute the derivatives.
To illustrate the point, if we restrain ourselves to using the simple iteration method as the root-finding algorithm, we can apply the chain rule and decompose the derivative in computing the derivative of $\bm{f}_{\gls{param}}$ for as many times as there were iterations until convergence, which can make practical applications impossible.

Luckily, we can exploit the fact that the output of the model ($D^{EQ}_{\gls{param}}\left( \bm{x} \right) = \bm{z}^{\star}$) is an equilibrium point of $\bm{f}_{\gls{param}}$, as done by \textcite{Bai2019}.
This implies that, in a neighborhood of the input vector $\bm{x}$, $D^{EQ}_{\gls{param}}$ is a \emph{parametrization} of $\bm{z}$ with respect to $\bm{x}$, i.e., \[
    \bm{f}_{\gls{param}}\left( \bm{x},D^{EQ}_{\gls{param}}(\bm{x}) \right) -D^{EQ}_{\gls{param}}\left( \bm{x} \right) = \bm{0}
\] is true. Then, by the definition of $\bm{g}$, it is also true that \[
\bm{g}\left( \bm{x}, D^{EQ}_{\gls{param}}\left( \bm{x} \right) \right) = \bm{0}
\] holds in a neighborhood of the input.
Therefore, by the \emph{implicit function theorem} (applied to $\bm{g}$ with $D^{EQ}_{\gls{param}}$ as the parametrization), we can write
\begin{equation}\label{eq:ift-g}
    \frac{d D^{EQ}_{\gls{param}}(\bm{x})}{d \gls{param}} = - \left[ \frac{d \bm{g}(\bm{x}, D^{EQ}_{\gls{param}}(\bm{x}))}{d \bm{z}} \right]^{-1} \frac{d \bm{g}(\bm{x}, D^{EQ}_{\gls{param}}(\bm{x}))}{d \gls{param}}
.\end{equation}
Now, by the definition of $\bm{g}$ we know that \[
    \frac{d \bm{g}(\bm{x}, \bm{z})}{d \bm{z}} = \frac{d \bm{f}_{\gls{param}}(\bm{x},\bm{z})}{d \bm{z}} - I 
\] and \[
\frac{d \bm{g}(\bm{x}, \bm{z})}{d \gls{param}} = \frac{d \bm{f}_{\gls{param}}(\bm{x},\bm{z})}{d \gls{param}}
.\] Therefore, we can rewrite \eqref{eq:ift-g} as
\begin{equation}\label{eq:deq-jacobian}
    \frac{d D^{EQ}_{\gls{param}}(\bm{x})}{d \gls{param}} = - \left[ \frac{d \bm{f}_{\gls{param}}(\bm{x},D^{EQ}_{\gls{param}}(\bm{x}))}{d \bm{z}} - I \right]^{-1} \frac{d \bm{f}_{\gls{param}}(\bm{x},D^{EQ}_{\gls{param}}(\bm{x}))}{d \gls{param}}
,\end{equation}
which allows us to compute the derivative of the \gls{DEQ} model \emph{regardless of how the equilibrium was computed}.
This makes it possible for us to use \emph{any} root-finding algorithm in the forward pass and still optimize the parameters of the model.
Furthermore, the implicit function theorem can also be applied to find the derivatives of the DEQ model with respect to the input vector, enabling back-propagation even if the "infinite depth" is just a component of a larger deep learning model.

\subsection{Implementation}\label{sec:deq-backward-implementation}

One may notice that trading the computation of the derivative of the root-finding algorithm for the computation of a matrix inverse (the Jacobian of $\bm{f}_{\gls{param}}$ with respect to $\bm{z}$) might not be a good idea.
Luckily, we do not actually need to compute the Jacobian of $D^{EQ}_{\gls{param}}$ to perform back-propagation.
As discussed in Section \ref{sec:backprop}, all we need is to compute a vector-Jacobian product of the form $\bm{u}^T\frac{d D^{EQ}_{\gls{param}}(\bm{x})}{d \gls{param}}$, which, in turn, can also be reduced to computing other vector-Jacobian products, following Equation \eqref{eq:deq-jacobian}.

It is clear that the challenge lies in computing the vector-Jacobian product $\bm{u}^T\left[ \frac{d \bm{f}_{\gls{param}}}{d \bm{z}} - I \right]^{-1}$ efficiently, as a matrix inversion is oftentimes too costly.
Yet, we can see that
\begin{align*}
    \bm{v}^T &= \bm{u}^T\left[ \frac{d \bm{f}_{\gls{param}}}{d \bm{z}} - I \right]^{-1} \\
    \implies \bm{u}^T &= \bm{v}^T\left[ \frac{d \bm{f}_{\gls{param}}}{d \bm{z}} - I \right] \\
		      &= \bm{v}^T \frac{d \bm{f}_{\gls{param}}}{d \bm{z}} - \bm{v}^T \\
    \implies \bm{v}^T &= \bm{v}^T \frac{d \bm{f}_{\gls{param}}}{d \bm{z}} - \bm{u}^T \tag{$*$}\label{eq:deq-backprop-eq}
.\end{align*}
The above equation tells us that, given $\bm{u}$ the result of the previous step, the challenging vector-Jacobian product can be computed by finding the equilibrium point of Equation \eqref{eq:deq-backprop-eq} on $\bm{v}$.
Therefore, a root-finding algorithm can be used to compute the result of the challenging vector-Jacobian product, just like in the forward pass.

